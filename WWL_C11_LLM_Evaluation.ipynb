{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuIklUt4lkixjgm3g5sQFR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C11_LLM_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk rouge-score --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ1SBbG5xpjw",
        "outputId": "af4dfa36-8efc-4fd6-ab88-a1941d2ee1d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity:\n",
        "\n",
        "Example: If the average perplexity is 20, this means the model is fairly good at predicting the next word. A lower value would be better, but 20 is reasonable for complex tasks.\n",
        "\n",
        "\n",
        "### BLEU:\n",
        "\n",
        "Example: If the average BLEU score is 0.45, this means the model's predictions have a good overlap with the reference sentences. A score closer to 0.5 or higher is often considered good.\n",
        "\n",
        "### ROUGE:\n",
        "\n",
        "Example: If the ROUGE-1 score is 0.6, ROUGE-2 is 0.4, and ROUGE-L is 0.5, this indicates that the generated text has a high overlap with the reference text at various granularities, which is a positive indication of performance."
      ],
      "metadata": {
        "id": "uxVPzUNxz9lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "_O2hs1MLzSsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "predictions = [\"the cat is on the mat\", \"there is a cat on the mat\"]\n",
        "references = [[\"the cat is on the mat\"], [\"there is a cat on the mat\"]]"
      ],
      "metadata": {
        "id": "PxgV9Ug2zU9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity Calculation\n",
        "def calculate_perplexity(predicted_sentence, reference_sentence):\n",
        "    ref_len = len(reference_sentence.split())\n",
        "    log_prob_sum = 0\n",
        "    for word in reference_sentence.split():\n",
        "        if word in predicted_sentence.split():\n",
        "            log_prob_sum += math.log(1 / (predicted_sentence.split().count(word) / len(predicted_sentence.split())))\n",
        "        else:\n",
        "            log_prob_sum += math.log(1 / len(predicted_sentence.split()))\n",
        "    return math.exp(log_prob_sum / ref_len)\n",
        "\n",
        "perplexities = [calculate_perplexity(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_perplexity = sum(perplexities) / len(perplexities)\n",
        "print(f\"Average Perplexity: {average_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR-f9jpszV8b",
        "outputId": "97a6f653-30e6-4f1f-e2da-ef47937d1549"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Perplexity: 5.881101577952299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score Calculation\n",
        "def calculate_bleu(predicted_sentence, reference_sentence):\n",
        "    return sentence_bleu([reference_sentence.split()], predicted_sentence.split())\n",
        "\n",
        "bleu_scores = [calculate_bleu(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU-Bq3e0zjfL",
        "outputId": "abb7d531-8911-4d8a-faee-f70a73ff7e70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def calculate_rouge(predicted_sentence, reference_sentence):\n",
        "    scores = scorer.score(reference_sentence, predicted_sentence)\n",
        "    return scores\n",
        "\n",
        "rouge_scores = [calculate_rouge(pred, ref[0]) for pred, ref in zip(predictions, references)]\n",
        "\n",
        "average_rouge = {\n",
        "    'rouge1': sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rouge2': sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "    'rougeL': sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "print(f\" Average ROUGE Scores: {average_rouge}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSMlLsJazmDh",
        "outputId": "9d224287-cc44-47d9-975a-e18f701e704a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average ROUGE Scores: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "inputs = [\n",
        "    \"Translate the following English text to French: 'Hello, how are you?'\",\n",
        "    \"Summarize the following text: 'The quick brown fox jumps over the lazy dog.'\"\n",
        "]\n",
        "references = [\n",
        "    [\"Bonjour, comment ça va?\"],\n",
        "    [\"The quick brown fox jumps over the lazy dog.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "R-7XSNqdxoh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3DOXSfnNz2Ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "outputId": "356a0a83-621b-4983-d78f-6c5dbf485cf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL_ID = \"gemini-1.5-flash-preview-0514\"  # @param {type:\"string\"}\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "AzeSs139AzWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "\n",
        "# load the model\n",
        "model = GenerativeModel(MODEL_ID, system_instruction=[ \"You are a helpful assistant.\",\"Your answer questions in a concise way\",],)\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig( temperature=0.9, top_k=32,)\n",
        "\n",
        "def generate_response(prompt,model=model):\n",
        "  contents = [prompt]\n",
        "  response = model.generate_content(contents, generation_config=generation_config,)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from GPT-3.5\n",
        "predictions = [get_response(input_text) for input_text in inputs]"
      ],
      "metadata": {
        "id": "AAtsTC5pzHji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tzkDA9FTz3TX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}