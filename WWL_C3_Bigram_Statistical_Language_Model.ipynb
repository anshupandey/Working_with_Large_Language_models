{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C3_Bigram_Statistical_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30b926c",
      "metadata": {
        "id": "a30b926c"
      },
      "source": [
        "# Training and Using a Bigram Statistical Language Model\n",
        "This notebook demonstrates how to train and use a bigram statistical language model using a dummy dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a5de983d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5de983d",
        "outputId": "e52ea81a-e81c-420a-9dbb-87eb15824b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba60bf75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba60bf75",
        "outputId": "4d5d8e82-9fd0-4ca8-af92-056801da98c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import bigrams\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "# Ensure the required NLTK packages are downloaded\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22113cb2",
      "metadata": {
        "id": "22113cb2"
      },
      "source": [
        "### Dummy Dataset\n",
        "Let's create a small dummy dataset of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "25a8e7f7",
      "metadata": {
        "id": "25a8e7f7"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'I love natural language processing',\n",
        "    'language models are fascinating',\n",
        "    'natural language processing is a complex field',\n",
        "    'I love machine learning',\n",
        "    'machine learning models can predict outcomes'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9d455d",
      "metadata": {
        "id": "fb9d455d"
      },
      "source": [
        "### Tokenization\n",
        "Tokenize the sentences into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f21cc4c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f21cc4c6",
        "outputId": "ac02107c-6a47-4083-d637-2796b3382219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sentences: [['I', 'love', 'natural', 'language', 'processing'], ['language', 'models', 'are', 'fascinating'], ['natural', 'language', 'processing', 'is', 'a', 'complex', 'field'], ['I', 'love', 'machine', 'learning'], ['machine', 'learning', 'models', 'can', 'predict', 'outcomes']]\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in corpus]\n",
        "print('Tokenized Sentences:', tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35108b62",
      "metadata": {
        "id": "35108b62"
      },
      "source": [
        "### Creating Bigram Model\n",
        "Create a bigram model and calculate the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c77fe217",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77fe217",
        "outputId": "fdb798f3-ea11-4c03-c2bd-22de0578a223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(I | None) = 0.4000\n",
            "P(language | None) = 0.2000\n",
            "P(natural | None) = 0.2000\n",
            "P(machine | None) = 0.2000\n",
            "P(love | I) = 1.0000\n",
            "P(natural | love) = 0.5000\n",
            "P(machine | love) = 0.5000\n",
            "P(language | natural) = 1.0000\n",
            "P(processing | language) = 0.6667\n",
            "P(models | language) = 0.3333\n",
            "P(None | processing) = 0.5000\n",
            "P(is | processing) = 0.5000\n",
            "P(are | models) = 0.5000\n",
            "P(can | models) = 0.5000\n",
            "P(fascinating | are) = 1.0000\n",
            "P(None | fascinating) = 1.0000\n",
            "P(a | is) = 1.0000\n",
            "P(complex | a) = 1.0000\n",
            "P(field | complex) = 1.0000\n",
            "P(None | field) = 1.0000\n",
            "P(learning | machine) = 1.0000\n",
            "P(None | learning) = 0.5000\n",
            "P(models | learning) = 0.5000\n",
            "P(predict | can) = 1.0000\n",
            "P(outcomes | predict) = 1.0000\n",
            "P(None | outcomes) = 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to hold our bigram counts\n",
        "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurrences (bigrams)\n",
        "for sentence in tokenized_sentences:\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        bigram_model[w1][w2] += 1\n",
        "\n",
        "# Convert counts to probabilities\n",
        "for w1 in bigram_model:\n",
        "    total_count = float(sum(bigram_model[w1].values()))\n",
        "    for w2 in bigram_model[w1]:\n",
        "        bigram_model[w1][w2] /= total_count\n",
        "\n",
        "# Display the bigram probabilities\n",
        "for w1 in bigram_model:\n",
        "    for w2 in bigram_model[w1]:\n",
        "        print(f'P({w2} | {w1}) = {bigram_model[w1][w2]:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1de276",
      "metadata": {
        "id": "5c1de276"
      },
      "source": [
        "### Generating Text Using the Bigram Model\n",
        "Now, let's use the trained bigram model to generate text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4bc0ddf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc0ddf4",
        "outputId": "178b14d0-2fe3-4ceb-bd3e-b15a21e8618b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: I love natural language models can predict outcomes\n"
          ]
        }
      ],
      "source": [
        "def generate_sentence(bigram_model, start_word, length=10):\n",
        "    current_word = start_word\n",
        "    sentence = [current_word]\n",
        "\n",
        "    for _ in range(length - 1):\n",
        "        next_word_candidates = list(bigram_model[current_word].keys())\n",
        "        next_word_probabilities = list(bigram_model[current_word].values())\n",
        "\n",
        "        if next_word_candidates:\n",
        "            next_word = random.choices(next_word_candidates, next_word_probabilities)[0]\n",
        "        else:\n",
        "            break\n",
        "\n",
        "        if next_word is None:\n",
        "            break\n",
        "\n",
        "        sentence.append(next_word)\n",
        "        current_word = next_word\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Generate a sentence starting with the word 'I'\n",
        "generated_sentence = generate_sentence(bigram_model, start_word='I')\n",
        "print('Generated Sentence:', generated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfbkcgbnhkKu"
      },
      "id": "kfbkcgbnhkKu",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}