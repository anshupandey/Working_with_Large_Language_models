{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C8_Implementing_Language_Model_with_Sentence_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9edd65b0",
      "metadata": {
        "id": "9edd65b0"
      },
      "source": [
        "# Exercise: Implementing a Language Model Using Sentence Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b6dfac",
      "metadata": {
        "id": "a0b6dfac"
      },
      "source": [
        "\n",
        "In this notebook, we will walk through the process of implementing a language model using Sentence Transformers. Sentence Transformers is a framework for sentence, paragraph, and image embeddings using BERT-like models. This framework makes it easy to generate embeddings for sentences and paragraphs which can then be used in various NLP tasks such as semantic search, clustering, and classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df9102c1",
      "metadata": {
        "id": "df9102c1"
      },
      "source": [
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before we start, make sure you have Python installed on your machine. You will also need to install the following packages:\n",
        "\n",
        "```bash\n",
        "pip install sentence-transformers\n",
        "pip install numpy\n",
        "pip install torch\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJMOj-yX90Oi",
        "outputId": "1297feb8-5594-4ca7-eadd-0015f0c0b46e"
      },
      "id": "pJMOj-yX90Oi",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2344425",
      "metadata": {
        "id": "c2344425"
      },
      "source": [
        "## Step 1: Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2a6ac84f",
      "metadata": {
        "id": "2a6ac84f",
        "outputId": "f74b13ca-b3b1-41d1-eb2c-0c626d3e4856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models,layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ee1cb1",
      "metadata": {
        "id": "97ee1cb1"
      },
      "source": [
        "## Step 2: Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "2a76bc8b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a76bc8b",
        "outputId": "3abb0d3c-bd6e-421a-d9e8-36751cdc0a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load a pre-trained Sentence Transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec8bd5a",
      "metadata": {
        "id": "3ec8bd5a"
      },
      "source": [
        "## Step 3: Encoding Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f2a89f82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2a89f82",
        "outputId": "61af3a66-5816-415b-e03d-5e338c513279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This is an example sentence.\n",
            "Embedding: [0.0981246  0.06781266 0.06252308 0.09508482 0.0366476 ]...\n",
            "\n",
            "Sentence: Each sentence is converted into a fixed-size vector.\n",
            "Embedding: [0.07381862 0.05663023 0.02722433 0.02096546 0.03240976]...\n",
            "\n",
            "Sentence: Sentence transformers are useful for various NLP tasks.\n",
            "Embedding: [-0.05859482  0.01600869  0.0545337   0.03032776  0.02576441]...\n",
            "\n",
            "Sentence: Setence Transformers can be used to represent sentences/documents as vectors\n",
            "Embedding: [-0.06910557  0.03544828 -0.03661951  0.00562107 -0.02724299]...\n",
            "\n",
            "Sentence: multiple powerful sentence transformer models are available at Hugging Face\n",
            "Embedding: [-0.04939263 -0.04512243  0.02534697  0.01934744 -0.00156226]...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Each sentence is converted into a fixed-size vector.\",\n",
        "    \"Sentence transformers are useful for various NLP tasks.\",\n",
        "    \"Setence Transformers can be used to represent sentences/documents as vectors\",\n",
        "    \"multiple powerful sentence transformer models are available at Hugging Face\"\n",
        "]\n",
        "\n",
        "# Encode the sentences into embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Display the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Embedding: {embedding[:5]}...\")  # Displaying first 5 dimensions for brevity\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23904b0",
      "metadata": {
        "id": "e23904b0"
      },
      "source": [
        "## Step 4: Finding Similar Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "39302e0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39302e0b",
        "outputId": "55db9090-da32-4a7c-e856-c11169ef42f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(384,)\n",
            "Sentence: This is an example sentence.\n",
            "Similarity Score: 0.3304\n",
            "\n",
            "Sentence: Each sentence is converted into a fixed-size vector.\n",
            "Similarity Score: 0.5168\n",
            "\n",
            "Sentence: Sentence transformers are useful for various NLP tasks.\n",
            "Similarity Score: 0.5501\n",
            "\n",
            "Sentence: Setence Transformers can be used to represent sentences/documents as vectors\n",
            "Similarity Score: 0.5568\n",
            "\n",
            "Sentence: multiple powerful sentence transformer models are available at Hugging Face\n",
            "Similarity Score: 0.4443\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a query sentence\n",
        "query_sentence = \"How are sentence embeddings generated?\"\n",
        "\n",
        "# Encode the query sentence\n",
        "query_embedding = model.encode(query_sentence)\n",
        "\n",
        "print(query_embedding.shape)\n",
        "\n",
        "# Compute cosine similarity between the query sentence and the other sentences\n",
        "cosine_scores = util.pytorch_cos_sim(query_embedding, embeddings)\n",
        "\n",
        "# Display the results\n",
        "for sentence, score in zip(sentences, cosine_scores[0]):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Similarity Score: {score.item():.4f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SHBxWNTvuxl6"
      },
      "id": "SHBxWNTvuxl6",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b8c92b5a",
      "metadata": {
        "id": "b8c92b5a"
      },
      "source": [
        "## Step 5: Clustering Sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Each sentence is converted into a fixed-size vector.\",\n",
        "    \"Sentence transformers are useful for various NLP tasks.\",\n",
        "    \"Setence Transformers can be used to represent sentences/documents as vectors\",\n",
        "    \"multiple powerful sentence transformer models are available at Hugging Face\",\n",
        "    \"Manila is a capital city of Philippines\",\n",
        "    \"The capital of the Philippines is Manila\",\n",
        "    \"Kuala Lumpur is the capital city of Malaysia\",\n",
        "    \"The capital of Malaysia is Kuala Lumpur\",\n",
        "    \"Beijing is the capital city of China\",\n",
        "    \"The capital of China is Beijing\",\n",
        "    \"London is the capital city of England\",\n",
        "    \"The capital of England is London\",\n",
        "    \"Paris is the capital city of France\",\n",
        "    \"The capital of France is Paris\",\n",
        "    \"Tokyo is the capital city of Japan\",\n",
        "    \"The capital of Japan is Tokyo\",\n",
        "    \"Sydney is the capital city of Australia\",\n",
        "    \"The capital of Australia is Sydney\",\n",
        "]\n",
        "\n",
        "# Encode the sentences into embeddings\n",
        "embeddings = model.encode(sentences)\n"
      ],
      "metadata": {
        "id": "6jqu5AbymsTa"
      },
      "id": "6jqu5AbymsTa",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "824a70ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "824a70ec",
        "outputId": "104dfed1-f379-4007-f178-5ef4f01ac000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "  - This is an example sentence.\n",
            "  - Each sentence is converted into a fixed-size vector.\n",
            "  - Sentence transformers are useful for various NLP tasks.\n",
            "  - Setence Transformers can be used to represent sentences/documents as vectors\n",
            "  - multiple powerful sentence transformer models are available at Hugging Face\n",
            "\n",
            "Cluster 1:\n",
            "  - Manila is a capital city of Philippines\n",
            "  - The capital of the Philippines is Manila\n",
            "  - Kuala Lumpur is the capital city of Malaysia\n",
            "  - The capital of Malaysia is Kuala Lumpur\n",
            "  - Beijing is the capital city of China\n",
            "  - The capital of China is Beijing\n",
            "  - London is the capital city of England\n",
            "  - The capital of England is London\n",
            "  - Paris is the capital city of France\n",
            "  - The capital of France is Paris\n",
            "  - Tokyo is the capital city of Japan\n",
            "  - The capital of Japan is Tokyo\n",
            "  - Sydney is the capital city of Australia\n",
            "  - The capital of Australia is Sydney\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Define the number of clusters\n",
        "num_clusters = 2\n",
        "\n",
        "# Perform K-Means clustering\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "# Display the clustering results\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    cluster_sentences = [sentences[j] for j in range(len(sentences)) if cluster_assignment[j] == i]\n",
        "    for sentence in cluster_sentences:\n",
        "        print(f\"  - {sentence}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0goiEYdOAPTW"
      },
      "id": "0goiEYdOAPTW",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b9ff60"
      },
      "source": [
        "## Step 6: Define a TensorFlow Function for Sentence Embeddings"
      ],
      "id": "19b9ff60"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get embeddings for all sentences\n",
        "def get_embeddings(sentences):\n",
        "    embeddings = model.encode(sentences)\n",
        "    return np.array(embeddings, dtype=np.float32)"
      ],
      "metadata": {
        "id": "u9hIbVxke-dg"
      },
      "id": "u9hIbVxke-dg",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278e1a79"
      },
      "source": [
        "## Step 7: Build LSTM Model"
      ],
      "id": "278e1a79"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "61de50d8"
      },
      "outputs": [],
      "source": [
        "# Function to build an LSTM model\n",
        "def build_model(embedding_dim, lstm_units, output_dim):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(embedding_dim,)),\n",
        "        layers.Reshape((1, embedding_dim)),\n",
        "        layers.LSTM(lstm_units, return_sequences=True),\n",
        "        layers.LSTM(lstm_units),\n",
        "        layers.Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "id": "61de50d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "989ef8e2"
      },
      "source": [
        "## Step 8: Prepare the Data"
      ],
      "id": "989ef8e2"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1BJ0eI9doIN9",
        "outputId": "12efd5de-cbaa-4cb4-87f2-349f5ae95926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1BJ0eI9doIN9",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ad3a2223"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example data preprocessing (assuming you have a dataset of sentences)\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Each sentence is converted into a fixed-size vector.\",\n",
        "    \"Sentence transformers are useful for various NLP tasks.\",\n",
        "    \"Setence Transformers can be used to represent sentences/documents as vectors\",\n",
        "    \"multiple powerful sentence transformer models are available at Hugging Face\",\n",
        "    \"Manila is a capital city of Philippines\",\n",
        "    \"The capital of the Philippines is Manila\",\n",
        "    \"Kuala Lumpur is the capital city of Malaysia\",\n",
        "    \"The capital of Malaysia is Kuala Lumpur\",\n",
        "    \"Beijing is the capital city of China\",\n",
        "    \"The capital of China is Beijing\",\n",
        "    \"London is the capital city of England\",\n",
        "    \"The capital of England is London\",\n",
        "    \"Paris is the capital city of France\",\n",
        "    \"The capital of France is Paris\",\n",
        "    \"Tokyo is the capital city of Japan\",\n",
        "    \"The capital of Japan is Tokyo\",\n",
        "    \"Sydney is the capital city of Australia\",\n",
        "    \"The capital of Australia is Sydney\",\n",
        "]\n",
        "\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "def get_data(sentences):\n",
        "  for sent in sentences:\n",
        "    words = word_tokenize(sent.strip().lower())\n",
        "    words = [w for w in words if w not in punctuations]\n",
        "    for i in range(len(words)-1):\n",
        "      x.append(\" \".join(words[0:i+1]))\n",
        "      y.append(words[i+1])\n",
        "\n",
        ""
      ],
      "id": "ad3a2223"
    },
    {
      "cell_type": "code",
      "source": [
        "get_data(sentences)\n",
        "len(x),len(y)"
      ],
      "metadata": {
        "id": "MKLPvEThojrl",
        "outputId": "3d3b13c3-f91f-4c22-b5fb-ea9005f7e610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MKLPvEThojrl",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116, 116)"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x)):\n",
        "  print(x[i],\"||\",y[i])"
      ],
      "metadata": {
        "id": "CCnFt90DolzT",
        "outputId": "346dc62a-86fc-44a5-e337-bcdc947c9546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CCnFt90DolzT",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this || is\n",
            "this is || an\n",
            "this is an || example\n",
            "this is an example || sentence\n",
            "each || sentence\n",
            "each sentence || is\n",
            "each sentence is || converted\n",
            "each sentence is converted || into\n",
            "each sentence is converted into || a\n",
            "each sentence is converted into a || fixed-size\n",
            "each sentence is converted into a fixed-size || vector\n",
            "sentence || transformers\n",
            "sentence transformers || are\n",
            "sentence transformers are || useful\n",
            "sentence transformers are useful || for\n",
            "sentence transformers are useful for || various\n",
            "sentence transformers are useful for various || nlp\n",
            "sentence transformers are useful for various nlp || tasks\n",
            "setence || transformers\n",
            "setence transformers || can\n",
            "setence transformers can || be\n",
            "setence transformers can be || used\n",
            "setence transformers can be used || to\n",
            "setence transformers can be used to || represent\n",
            "setence transformers can be used to represent || sentences/documents\n",
            "setence transformers can be used to represent sentences/documents || as\n",
            "setence transformers can be used to represent sentences/documents as || vectors\n",
            "multiple || powerful\n",
            "multiple powerful || sentence\n",
            "multiple powerful sentence || transformer\n",
            "multiple powerful sentence transformer || models\n",
            "multiple powerful sentence transformer models || are\n",
            "multiple powerful sentence transformer models are || available\n",
            "multiple powerful sentence transformer models are available || at\n",
            "multiple powerful sentence transformer models are available at || hugging\n",
            "multiple powerful sentence transformer models are available at hugging || face\n",
            "manila || is\n",
            "manila is || a\n",
            "manila is a || capital\n",
            "manila is a capital || city\n",
            "manila is a capital city || of\n",
            "manila is a capital city of || philippines\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || the\n",
            "the capital of the || philippines\n",
            "the capital of the philippines || is\n",
            "the capital of the philippines is || manila\n",
            "kuala || lumpur\n",
            "kuala lumpur || is\n",
            "kuala lumpur is || the\n",
            "kuala lumpur is the || capital\n",
            "kuala lumpur is the capital || city\n",
            "kuala lumpur is the capital city || of\n",
            "kuala lumpur is the capital city of || malaysia\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || malaysia\n",
            "the capital of malaysia || is\n",
            "the capital of malaysia is || kuala\n",
            "the capital of malaysia is kuala || lumpur\n",
            "beijing || is\n",
            "beijing is || the\n",
            "beijing is the || capital\n",
            "beijing is the capital || city\n",
            "beijing is the capital city || of\n",
            "beijing is the capital city of || china\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || china\n",
            "the capital of china || is\n",
            "the capital of china is || beijing\n",
            "london || is\n",
            "london is || the\n",
            "london is the || capital\n",
            "london is the capital || city\n",
            "london is the capital city || of\n",
            "london is the capital city of || england\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || england\n",
            "the capital of england || is\n",
            "the capital of england is || london\n",
            "paris || is\n",
            "paris is || the\n",
            "paris is the || capital\n",
            "paris is the capital || city\n",
            "paris is the capital city || of\n",
            "paris is the capital city of || france\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || france\n",
            "the capital of france || is\n",
            "the capital of france is || paris\n",
            "tokyo || is\n",
            "tokyo is || the\n",
            "tokyo is the || capital\n",
            "tokyo is the capital || city\n",
            "tokyo is the capital city || of\n",
            "tokyo is the capital city of || japan\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || japan\n",
            "the capital of japan || is\n",
            "the capital of japan is || tokyo\n",
            "sydney || is\n",
            "sydney is || the\n",
            "sydney is the || capital\n",
            "sydney is the capital || city\n",
            "sydney is the capital city || of\n",
            "sydney is the capital city of || australia\n",
            "the || capital\n",
            "the capital || of\n",
            "the capital of || australia\n",
            "the capital of australia || is\n",
            "the capital of australia is || sydney\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xvec = get_embeddings(x)\n",
        "print(xvec.shape)\n",
        "# Tokenize the sentences\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "y_encoded = tokenizer.texts_to_sequences(y)\n",
        "print(len(y_encoded))"
      ],
      "metadata": {
        "id": "ka5JzDjfq2ld",
        "outputId": "43930ec3-5e27-4cdc-e0c3-d7e90b2bbc3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ka5JzDjfq2ld",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(116, 384)\n",
            "116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "g_p4ooK7q70k",
        "outputId": "d4b6baeb-b97b-4d9e-d465-1eb6bd46a301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g_p4ooK7q70k",
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is': 1, 'capital': 2, 'of': 3, 'the': 4, 'city': 5, 'sentence': 6, 'a': 7, 'transformers': 8, 'are': 9, 'manila': 10, 'philippines': 11, 'kuala': 12, 'lumpur': 13, 'malaysia': 14, 'beijing': 15, 'china': 16, 'london': 17, 'england': 18, 'paris': 19, 'france': 20, 'tokyo': 21, 'japan': 22, 'sydney': 23, 'australia': 24, 'this': 25, 'an': 26, 'example': 27, 'each': 28, 'converted': 29, 'into': 30, 'fixed': 31, 'size': 32, 'vector': 33, 'useful': 34, 'for': 35, 'various': 36, 'nlp': 37, 'tasks': 38, 'setence': 39, 'can': 40, 'be': 41, 'used': 42, 'to': 43, 'represent': 44, 'sentences': 45, 'documents': 46, 'as': 47, 'vectors': 48, 'multiple': 49, 'powerful': 50, 'transformer': 51, 'models': 52, 'available': 53, 'at': 54, 'hugging': 55, 'face': 56}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)\n",
        "print(y_encoded)"
      ],
      "metadata": {
        "id": "vsYKtzzvrARy",
        "outputId": "cbb7d791-f922-473f-8d5e-220ec77e0f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vsYKtzzvrARy",
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is', 'an', 'example', 'sentence', 'sentence', 'is', 'converted', 'into', 'a', 'fixed-size', 'vector', 'transformers', 'are', 'useful', 'for', 'various', 'nlp', 'tasks', 'transformers', 'can', 'be', 'used', 'to', 'represent', 'sentences/documents', 'as', 'vectors', 'powerful', 'sentence', 'transformer', 'models', 'are', 'available', 'at', 'hugging', 'face', 'is', 'a', 'capital', 'city', 'of', 'philippines', 'capital', 'of', 'the', 'philippines', 'is', 'manila', 'lumpur', 'is', 'the', 'capital', 'city', 'of', 'malaysia', 'capital', 'of', 'malaysia', 'is', 'kuala', 'lumpur', 'is', 'the', 'capital', 'city', 'of', 'china', 'capital', 'of', 'china', 'is', 'beijing', 'is', 'the', 'capital', 'city', 'of', 'england', 'capital', 'of', 'england', 'is', 'london', 'is', 'the', 'capital', 'city', 'of', 'france', 'capital', 'of', 'france', 'is', 'paris', 'is', 'the', 'capital', 'city', 'of', 'japan', 'capital', 'of', 'japan', 'is', 'tokyo', 'is', 'the', 'capital', 'city', 'of', 'australia', 'capital', 'of', 'australia', 'is', 'sydney']\n",
            "[[1], [26], [27], [6], [6], [1], [29], [30], [7], [31, 32], [33], [8], [9], [34], [35], [36], [37], [38], [8], [40], [41], [42], [43], [44], [45, 46], [47], [48], [50], [6], [51], [52], [9], [53], [54], [55], [56], [1], [7], [2], [5], [3], [11], [2], [3], [4], [11], [1], [10], [13], [1], [4], [2], [5], [3], [14], [2], [3], [14], [1], [12], [13], [1], [4], [2], [5], [3], [16], [2], [3], [16], [1], [15], [1], [4], [2], [5], [3], [18], [2], [3], [18], [1], [17], [1], [4], [2], [5], [3], [20], [2], [3], [20], [1], [19], [1], [4], [2], [5], [3], [22], [2], [3], [22], [1], [21], [1], [4], [2], [5], [3], [24], [2], [3], [24], [1], [23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded = [k[0] for k in y_encoded]\n",
        "y_encoded = np.array(y_encoded)\n",
        "y_encoded.shape"
      ],
      "metadata": {
        "id": "s8S1U7qEruv1",
        "outputId": "a61048d4-700f-4539-c6d4-a4238b2185d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s8S1U7qEruv1",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(116,)"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y_encoded, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "gXdEEUfCkYSs"
      },
      "id": "gXdEEUfCkYSs",
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbde62a7"
      },
      "source": [
        "## Step 6: Train the Model"
      ],
      "id": "fbde62a7"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = embeddings.shape[1]\n",
        "lstm_units = 128\n",
        "\n",
        "language_model = build_model(embedding_dim, lstm_units, vocab_size)\n",
        "language_model.fit(xvec, y_encoded, epochs=10, batch_size=2)"
      ],
      "metadata": {
        "id": "yLC1eePfrVif",
        "outputId": "5283b659-525c-41bd-a664-7eab8ab2a388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yLC1eePfrVif",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "58/58 [==============================] - 5s 15ms/step - loss: 3.9984 - accuracy: 0.1466\n",
            "Epoch 2/10\n",
            "58/58 [==============================] - 1s 16ms/step - loss: 3.2973 - accuracy: 0.1810\n",
            "Epoch 3/10\n",
            "58/58 [==============================] - 1s 16ms/step - loss: 2.8977 - accuracy: 0.2500\n",
            "Epoch 4/10\n",
            "58/58 [==============================] - 1s 15ms/step - loss: 2.6977 - accuracy: 0.2672\n",
            "Epoch 5/10\n",
            "58/58 [==============================] - 1s 14ms/step - loss: 2.5360 - accuracy: 0.3017\n",
            "Epoch 6/10\n",
            "58/58 [==============================] - 1s 17ms/step - loss: 2.3845 - accuracy: 0.3103\n",
            "Epoch 7/10\n",
            "58/58 [==============================] - 1s 19ms/step - loss: 2.2719 - accuracy: 0.3448\n",
            "Epoch 8/10\n",
            "58/58 [==============================] - 1s 18ms/step - loss: 2.1384 - accuracy: 0.3966\n",
            "Epoch 9/10\n",
            "58/58 [==============================] - 1s 18ms/step - loss: 1.9950 - accuracy: 0.4483\n",
            "Epoch 10/10\n",
            "58/58 [==============================] - 1s 16ms/step - loss: 1.8465 - accuracy: 0.4655\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7af2d564ad40>"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1623b7d"
      },
      "source": [
        "## Step 7: Predict Next Word"
      ],
      "id": "a1623b7d"
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "c1b354ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac5a6dcf-4013-4e30-c2de-8e23748185cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word prediction: a\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to predict the next word given a model, tokenizer, and sentence\n",
        "def predict_next_word(language_model, tokenizer, sentence):\n",
        "    embedding = get_embeddings([sentence])\n",
        "    #print(embedding.shape)\n",
        "    prediction = language_model.predict(embedding, verbose=0)\n",
        "    #print(prediction)\n",
        "    predicted_word_index = np.argmax(prediction[0])\n",
        "    #print(predicted_word_index)\n",
        "    predicted_word = tokenizer.index_word[predicted_word_index+1]\n",
        "    return predicted_word\n",
        "\n",
        "# Example prediction\n",
        "sentence = \"This is an example\"\n",
        "predicted_word = predict_next_word(language_model, tokenizer, sentence)\n",
        "print(f\"Next word prediction: {predicted_word}\")\n"
      ],
      "id": "c1b354ca"
    },
    {
      "cell_type": "code",
      "source": [
        "userinput = \"This\"\n",
        "generation_length = 10\n",
        "\n",
        "for i in range(generation_length):\n",
        "    predicted_word = predict_next_word(language_model, tokenizer, userinput)\n",
        "    userinput += \" \" + predicted_word\n",
        "    print(userinput)"
      ],
      "metadata": {
        "id": "Rojttzn8szMd",
        "outputId": "f6d2c2ec-22cc-4955-dc17-f1081ae39de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Rojttzn8szMd",
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This capital\n",
            "This capital the\n",
            "This capital the the\n",
            "This capital the the the\n",
            "This capital the the the of\n",
            "This capital the the the of the\n",
            "This capital the the the of the the\n",
            "This capital the the the of the the of\n",
            "This capital the the the of the the of of\n",
            "This capital the the the of the the of of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7784becb",
      "metadata": {
        "id": "7784becb"
      },
      "source": [
        "\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, we have covered the basics of implementing a language model using Sentence Transformers. We have shown how to encode sentences into embeddings, find similar sentences using cosine similarity, and cluster sentences using K-Means clustering. Sentence Transformers provide a powerful and flexible way to work with sentence embeddings for various NLP tasks.\n",
        "\n",
        "You can further explore Sentence Transformers by trying out different models, tweaking hyperparameters, and applying these techniques to your specific use cases. Happy coding!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2ohVsCEAROX"
      },
      "id": "c2ohVsCEAROX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}