{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C24_MakerSuite_Getting_started_Gemini_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Lv3UtGCFH4"
      },
      "source": [
        "# Gemini API: List models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh9D-DvWSuqq"
      },
      "source": [
        "This notebook demonstrates how to list the models that are available for you to use in the Gemini API, and how to find details about a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i755jXzS5kLN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "1SSh3VnQr9qV",
        "outputId": "ddc83565-222f-4284-e427-e03f17572475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "49H9jQPO_TJ9"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ol10W6Q_Y-s"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8PXsFZBQ_XA5"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Al4lFhNB22n"
      },
      "source": [
        "## List models\n",
        "\n",
        "Use `list_models()` to see what models are available. These models support `generateContent`, the main method used for prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3wE76b_gBn2k",
        "outputId": "7cb1547f-692a-4fe4-d941-b831291a3a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if \"generateContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlguLt1yKET9"
      },
      "source": [
        "These models support `embedContent`, used for embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lQmlIpr5JHqz",
        "outputId": "84500b68-7b33-438d-b9d6-2e19c0c4c61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if \"embedContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFJAyDD9QVrC"
      },
      "source": [
        "## Find details about a model\n",
        "\n",
        "You can see more details about a model, including the `input_token_limit` and `output_token_limit` as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BYYxVE4ZnoGy",
        "outputId": "e238c679-121a-42e3-e418-543ca50b133c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n"
          ]
        }
      ],
      "source": [
        "for m in genai.list_models():\n",
        "    if m.name == \"models/gemini-1.5-flash\":\n",
        "        print(m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00a56cb21953"
      },
      "source": [
        "## Get model\n",
        "\n",
        "Use `get_model()` to retrieve the specific details of a model. You can iterate over all available models using `list_models()`, but if you already know the model name you can retrieve it directly with `get_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6786759016dc",
        "outputId": "ec4376b5-8833-416f-d5fb-8c4a113c84d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ],
      "source": [
        "model_info = genai.get_model(\"models/aqa\")\n",
        "print(model_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.0-pro-latest')"
      ],
      "metadata": {
        "id": "nF01_jipth8p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Write a poem on How beautiful city Manila is.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "IeM0C1v2nM02",
        "outputId": "090c48f0-8314-450f-ade1-82a91e0b77c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Ode to Manila's Brilliance**\n",
            "\n",
            "In the heart of Luzon's embrace,\n",
            "A city of vibrant grace,\n",
            "Manila, where beauty resides,\n",
            "A tapestry of wonders, where history abides.\n",
            "\n",
            "Skyscrapers rise, sleek and tall,\n",
            "Reflecting the sun's ethereal call,\n",
            "A symphony of steel and glass,\n",
            "A modern masterpiece, built to last.\n",
            "\n",
            "Along the bustling Pasig's flow,\n",
            "Historic churches, a timeless glow,\n",
            "Intramuros' ancient walls, a fortress strong,\n",
            "Whispering tales of the past, where battles were fought.\n",
            "\n",
            "Chinatowns vibrant, a culinary delight,\n",
            "Aromatic dishes, a feast for sight,\n",
            "Binondo's streets, a vibrant maze,\n",
            "A melting pot of cultures, a harmonious gaze.\n",
            "\n",
            "Rizal Park, a tranquil green,\n",
            "Where heroes stand, their legacy serene,\n",
            "The Luneta's sunset, a sight to behold,\n",
            "A canvas painted with hues of gold.\n",
            "\n",
            "Baywalk's promenade, a romantic stroll,\n",
            "As lovers whisper sweet nothings, their hearts whole,\n",
            "The sound of waves, a calming refrain,\n",
            "A symphony of nature, as the city turns off its strain.\n",
            "\n",
            "Museums and galleries, a treasure trove,\n",
            "Where art and history intertwine and prove,\n",
            "The National Museum, a cultural gem,\n",
            "Unveiling the Philippines' rich past, a timeless anthem.\n",
            "\n",
            "Manila's beauty knows no bounds,\n",
            "A city that captivates, where wonder is found,\n",
            "A vibrant metropolis, a cultural delight,\n",
            "A testament to history, a beacon of light.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq7i5FAwCe1v"
      },
      "source": [
        "## Learning more\n",
        "\n",
        "* To learn how use a model for prompting, see the [Prompting](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Prompting.ipynb) quickstart.\n",
        "\n",
        "* To learn how use a model for embedding, see the [Embedding](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb) quickstart.\n",
        "\n",
        "* For more information on models, visit the [Gemini models](https://ai.google.dev/models/gemini) documentation."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q-py3QRQsB8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}