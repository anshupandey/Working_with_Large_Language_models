{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV7hdSzVjxVDIX88+JWDDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C10_Accessing_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. OpenAI GPT"
      ],
      "metadata": {
        "id": "CCN-QkJL_dEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KB85L9Fs6jMM",
        "outputId": "c5d4bd3a-1627-4967-9d36-1a58643f370b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/327.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/327.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "noj0x1VU6zlA"
      },
      "outputs": [],
      "source": [
        "api_key = \"sk-proj-xxxxxxxxxxxxxx\" # replace this with your openai api key\n",
        "model_name = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BC0L6Sa6LkGm"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y_ba7QKuLkGm",
        "outputId": "356ca2d1-2ff2-47c2-b3f6-a0a5844cf4a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9efTXlRbGiLpAGz8DiDMaQNeuKmu2\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"You can use the `os` module in Python to list all files in a directory. Here is an example code snippet that demonstrates how to do this:\\n\\n```python\\nimport os\\n\\n# Specify the directory path\\ndirectory = '/path/to/directory'\\n\\n# Get a list of all files in the directory\\nfiles = os.listdir(directory)\\n\\n# Print the list of files\\nfor file in files:\\n    print(file)\\n```\\n\\nMake sure to replace `'/path/to/directory'` with the actual path of the directory you want to list files from.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1719479579,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 112,\n",
            "    \"prompt_tokens\": 19,\n",
            "    \"total_tokens\": 131\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=model_name,  # e.g. gpt-35-instant\n",
        "    max_tokens=500,\n",
        "    temperature=0.9,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(result.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-cNZj6nuLkGm",
        "outputId": "965bd391-e6a5-41fd-8534-4c4d595b44d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can use the `os` module in Python to list all files in a directory. Here is an example code snippet that demonstrates how to do this:\n",
            "\n",
            "```python\n",
            "import os\n",
            "\n",
            "# Specify the directory path\n",
            "directory = '/path/to/directory'\n",
            "\n",
            "# Get a list of all files in the directory\n",
            "files = os.listdir(directory)\n",
            "\n",
            "# Print the list of files\n",
            "for file in files:\n",
            "    print(file)\n",
            "```\n",
            "\n",
            "Make sure to replace `'/path/to/directory'` with the actual path of the directory you want to list files from.\n"
          ]
        }
      ],
      "source": [
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0WAxkVWZ7LRi"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "def generate_response(prompt,model=model_name):\n",
        "  messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=0\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLgNfQbz7xpQ",
        "outputId": "12060cf2-d9d2-4ad4-9a1a-381a5cddfb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manila, city of lights and dreams,\n",
            "Where the sun sets in a blaze of golden beams.\n",
            "Skyscrapers reach up to touch the sky,\n",
            "Reflecting the beauty that catches the eye.\n",
            "\n",
            "The streets are alive with a vibrant energy,\n",
            "People bustling about, full of synergy.\n",
            "From the historic walls of Intramuros,\n",
            "To the modern malls where luxury flows.\n",
            "\n",
            "The Pasig River flows through the heart,\n",
            "A lifeline that sets the city apart.\n",
            "Green spaces like Rizal Park provide respite,\n",
            "From the hustle and bustle of city life.\n",
            "\n",
            "The food, oh the food, a culinary delight,\n",
            "From street food stalls to fine dining at night.\n",
            "The flavors of Manila are a feast for the senses,\n",
            "A melting pot of cultures, with no pretenses.\n",
            "\n",
            "And when the night falls, the city comes alive,\n",
            "With music and laughter that will revive.\n",
            "Manila, a city of contrasts and beauty,\n",
            "A place where the past and present meet seamlessly.\n",
            "\n",
            "So here's to Manila, a city so grand,\n",
            "With a beauty that's impossible to withstand.\n",
            "A place where dreams are made and fulfilled,\n",
            "A city that will forever be cherished and willed.\n"
          ]
        }
      ],
      "source": [
        "response = generate_response(\"Write a poem on how beautiful city Manila is\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"Write a poem on how beautiful city Manila is\",model='gpt-4o')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "XXc38RMb_0lk",
        "outputId": "17fb9a91-397b-4044-fe46-76859dfa93a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the heart of the archipelago, where the sun kisses the sea,\n",
            "Lies a city of vibrant hues, where history and future meet.\n",
            "Manila, oh Manila, with your streets alive and free,\n",
            "A tapestry of cultures, where every corner is a treat.\n",
            "\n",
            "From the ancient walls of Intramuros, echoing tales of yore,\n",
            "To the bustling markets of Divisoria, where treasures are in store.\n",
            "The Pasig River winds its way, a lifeline through the years,\n",
            "Witness to the laughter, the struggles, and the tears.\n",
            "\n",
            "Rizal Park stands proud, a tribute to a hero's dream,\n",
            "Where children play and lovers stroll, beneath the moon's soft gleam.\n",
            "The jeepneys paint the avenues with colors bold and bright,\n",
            "A symbol of resilience, in the day and through the night.\n",
            "\n",
            "Binondo's streets are fragrant with the scent of dim sum steam,\n",
            "While Malate's nightlife pulses with a vibrant, youthful gleam.\n",
            "The sunsets over Manila Bay, a canvas of gold and red,\n",
            "A daily masterpiece that leaves the soul well-fed.\n",
            "\n",
            "In every smile, in every song, in every dance and cheer,\n",
            "Manila's spirit shines so strong, a beacon far and near.\n",
            "A city of contrasts, of old and new, of sorrow and delight,\n",
            "Manila, oh Manila, you are a wondrous sight.\n",
            "\n",
            "So here's to you, dear city, with your heart so warm and true,\n",
            "A place where dreams are woven, and every day feels new.\n",
            "Manila, oh Manila, with your beauty vast and grand,\n",
            "You are the pearl of the orient, a gem in this fair land.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Anthropic"
      ],
      "metadata": {
        "id": "ICEIuJJ2_32S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0NoVlXD_7hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5c0bed1d-553c-47ea-9d8a-c5295fc4841e"
      },
      "outputs": [],
      "source": [
        "!pip install -q anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd9e939-154a-4c74-a1e2-8fe48971f955"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b3856fcd-437d-4223-8ac8-2e281de6bf15"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "from anthropic import HUMAN_PROMPT, AI_PROMPT\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bf18ec-08d5-4885-9555-9055fb6536db",
        "outputId": "7872d262-a5d8-4673-9436-ef508eb845cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "ANTHROPIC_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9e2c880f-70cb-4d8a-b60d-e43bddef9732"
      },
      "outputs": [],
      "source": [
        "client = anthropic.Anthropic(\n",
        "    api_key=ANTHROPIC_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "a0c213b9-f98f-4ad5-9303-3d1711d264e9"
      },
      "outputs": [],
      "source": [
        "model_1 = 'claude-instant-1.2'\n",
        "model_2 = 'claude-2.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5da2ccde-3c7e-4719-a9c4-7c22949db94b"
      },
      "outputs": [],
      "source": [
        "def generate_response(PROMPT, model=\"claude-2.1\"):\n",
        "\n",
        "    completion = client.completions.create(\n",
        "        model = model,\n",
        "        max_tokens_to_sample = 300,\n",
        "        prompt = f\"{HUMAN_PROMPT} {PROMPT} {AI_PROMPT}\")\n",
        "\n",
        "    return completion.completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"Write a python code to print 'Hello World'\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "FtlyXZhvAzB2",
        "outputId": "2aa94386-3124-4be8-d6d1-032b1d5646fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bd05ae1bf2ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Write a python code to print 'Hello World'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-5c3b6f30b857>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(PROMPT, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROMPT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-2.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     completion = client.completions.create(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_tokens_to_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens_to_sample, model, prompt, metadata, stop_sequences, stream, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     ) -> Completion | Stream[Completion]:\n\u001b[0;32m--> 374\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;34m\"/v1/complete\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         )\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 931\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "PtCRchzQA7cm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKBdp7w5A9Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tFy3H3aPgx12",
        "outputId": "356a0a83-621b-4983-d78f-6c5dbf485cf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL_ID = \"gemini-1.5-flash-preview-0514\"  # @param {type:\"string\"}\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "AzeSs139AzWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "\n",
        "# load the model\n",
        "model = GenerativeModel(MODEL_ID, system_instruction=[ \"You are a helpful assistant.\",\"Your answer questions in a concise way\",],)\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig( temperature=0.9, top_k=32,)\n",
        "\n",
        "def generate_response(prompt,model=model):\n",
        "  contents = [prompt]\n",
        "  response = model.generate_content(contents, generation_config=generation_config,)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"Write a python code to print 'Hello World'\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_OsroYzHBNnE",
        "outputId": "9fbd6f99-95d0-4841-dbd7-09d4ba1f73e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "print(\"Hello World\")\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PaLM 2 Chat Bison"
      ],
      "metadata": {
        "id": "WjIcOx69Ayce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model_ID = \"chat-bison@002\"\n",
        "# load the model\n",
        "model = GenerativeModel(MODEL_ID, system_instruction=[ \"You are a helpful assistant.\",\"Your answer questions in a concise way\",],)\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig( temperature=0.9, top_k=32,)\n",
        "\n",
        "def generate_response(prompt,model=model):\n",
        "  contents = [prompt]\n",
        "  response = model.generate_content(contents, generation_config=generation_config,)\n",
        "  return response.text\n",
        "\n",
        "response = generate_response(\"Write a poem on how beautiful city Manila is\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "JZM2hAxzAxTc",
        "outputId": "af2748c7-eee7-4ca4-d734-bb77685c371b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manila's heart, a vibrant beat,\n",
            "Where ancient walls and modern meet.\n",
            "Intramuros whispers tales of yore,\n",
            "Beneath a sun-kissed, azure shore.\n",
            "\n",
            "From jeepneys' honks to market cries,\n",
            "A symphony of life, it flies.\n",
            "The Pasig River, a silver thread,\n",
            "Through bustling streets, its waters spread.\n",
            "\n",
            "A tapestry of culture, rich and deep,\n",
            "From Spanish grace to Filipino leap.\n",
            "The Bay's embrace, a gentle hold,\n",
            "Where stories bloom, both new and old.\n",
            "\n",
            "Manila, a city, strong and bright,\n",
            "A beacon of hope, in day and night.\n",
            "Its beauty lies in every face,\n",
            "A city with a vibrant grace. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwEryKUdBZM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}