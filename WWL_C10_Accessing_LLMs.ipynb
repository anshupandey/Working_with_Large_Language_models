{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyTzFfkUBA80bOBFM38va3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Working_with_Large_Language_models/blob/main/WWL_C10_Accessing_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. OpenAI GPT"
      ],
      "metadata": {
        "id": "CCN-QkJL_dEs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB85L9Fs6jMM",
        "outputId": "4b00c7ab-90c5-490c-d944-9bdb0c9976a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYpMXYoQLkGl"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noj0x1VU6zlA"
      },
      "outputs": [],
      "source": [
        "api_key = \"XXXXXXXXXXXXXXXX488252f\" # replace this with your openai api key\n",
        "model_name = \"gpt-3.5-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC0L6Sa6LkGm"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_ba7QKuLkGm",
        "outputId": "057806a5-d071-49ba-e612-88ff1cea45fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8YfkSX23hoFjv0FN7OTbrdzEkpf84\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"You can use the `os` module in Python to output all files in a directory. Here's an example of how to do it:\\n\\n```python\\nimport os\\n\\n# Replace 'path_to_directory' with the actual path to the directory\\ndirectory = 'path_to_directory'\\n\\n# Get a list of all files in the directory\\nfiles = os.listdir(directory)\\n\\n# Output all files in the directory\\nfor file in files:\\n    print(file)\\n```\\n\\nThis code will list all files in the directory specified by replacing `'path_to_directory'` with the actual path.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      },\n",
            "      \"content_filter_results\": {\n",
            "        \"hate\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"self_harm\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"sexual\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"violence\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1703274324,\n",
            "  \"model\": \"gpt-35-turbo\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": \"fp_50a4261de5\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 115,\n",
            "    \"prompt_tokens\": 19,\n",
            "    \"total_tokens\": 134\n",
            "  },\n",
            "  \"prompt_filter_results\": [\n",
            "    {\n",
            "      \"prompt_index\": 0,\n",
            "      \"content_filter_results\": {\n",
            "        \"hate\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"self_harm\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"sexual\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"violence\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "result = client.chat.completions.create(\n",
        "    model=model_name,  # e.g. gpt-35-instant\n",
        "    max_tokens=500,\n",
        "    temperature=0.9,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I output all files in a directory using Python?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "print(result.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cNZj6nuLkGm",
        "outputId": "f1924b39-fc26-4c99-a737-776b5db27949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can use the `os` module in Python to output all files in a directory. Here's an example of how to do it:\n",
            "\n",
            "```python\n",
            "import os\n",
            "\n",
            "# Replace 'path_to_directory' with the actual path to the directory\n",
            "directory = 'path_to_directory'\n",
            "\n",
            "# Get a list of all files in the directory\n",
            "files = os.listdir(directory)\n",
            "\n",
            "# Output all files in the directory\n",
            "for file in files:\n",
            "    print(file)\n",
            "```\n",
            "\n",
            "This code will list all files in the directory specified by replacing `'path_to_directory'` with the actual path.\n"
          ]
        }
      ],
      "source": [
        "print(result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WAxkVWZ7LRi"
      },
      "outputs": [],
      "source": [
        "# creating a function to get outcome\n",
        "\n",
        "\n",
        "def generate_response(prompt,model=model_name):\n",
        "  messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "  response = client.chat.completions.create(\n",
        "      model = model,\n",
        "      messages = messages,\n",
        "      temperature=0\n",
        "  )\n",
        "  return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tLgNfQbz7xpQ",
        "outputId": "c4cd65e2-dd28-49b6-ecbf-efb48d0c0e48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"```python\\nprint('Hello World')\\n```\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = generate_response(\"Write a python code to print 'Hello World'\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Anthropic"
      ],
      "metadata": {
        "id": "ICEIuJJ2_32S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0NoVlXD_7hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c0bed1d-553c-47ea-9d8a-c5295fc4841e",
        "outputId": "d3170f66-d4dd-41b3-da1e-e6e0bc094ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m870.8/870.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd9e939-154a-4c74-a1e2-8fe48971f955"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3856fcd-437d-4223-8ac8-2e281de6bf15"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "from anthropic import HUMAN_PROMPT, AI_PROMPT\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bf18ec-08d5-4885-9555-9055fb6536db",
        "outputId": "29ca2746-7c11-46e0-893e-03d9535216da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "ANTHROPIC_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e2c880f-70cb-4d8a-b60d-e43bddef9732"
      },
      "outputs": [],
      "source": [
        "client = anthropic.Anthropic(\n",
        "    api_key=ANTHROPIC_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0c213b9-f98f-4ad5-9303-3d1711d264e9"
      },
      "outputs": [],
      "source": [
        "model_1 = 'claude-instant-1.2'\n",
        "model_2 = 'claude-2.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5da2ccde-3c7e-4719-a9c4-7c22949db94b"
      },
      "outputs": [],
      "source": [
        "def generate_response(PROMPT, model=\"claude-2.1\"):\n",
        "\n",
        "    completion = client.completions.create(\n",
        "        model = model,\n",
        "        max_tokens_to_sample = 300,\n",
        "        prompt = f\"{HUMAN_PROMPT} {PROMPT} {AI_PROMPT}\")\n",
        "\n",
        "    return completion.completion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"Write a python code to print 'Hello World'\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "FtlyXZhvAzB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Gemini 1.5 Flash"
      ],
      "metadata": {
        "id": "PtCRchzQA7cm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PKBdp7w5A9Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No17Cw5hgx12"
      },
      "source": [
        "### Install Vertex AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFy3H3aPgx12"
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF4l8DTdWgPY"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqwi-5ufWp_B"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"jrproject-402905\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL_ID = \"gemini-1.5-flash-preview-0514\"  # @param {type:\"string\"}\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lslYAvw37JGQ"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerationConfig, GenerativeModel\n",
        "\n",
        "# load the model\n",
        "model = GenerativeModel(MODEL_ID, system_instruction=[ \"You are a helpful assistant.\",\"Your answer questions in a concise way\",],)\n",
        "\n",
        "# Set model parameters\n",
        "generation_config = GenerationConfig( temperature=0.9, top_k=32,)\n",
        "\n",
        "def generate_response(prompt,model=model):\n",
        "  contents = [prompt]\n",
        "  response = model.generate_content(contents, generation_config=generation_config,)\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(\"Write a python code to print 'Hello World'\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "_OsroYzHBNnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}